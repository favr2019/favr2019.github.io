<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <title>NIME Workshop on Audio-first VR</title>
  <link rel="stylesheet" type="text/css" href="style.css">

</head>

<body>
  <div id="title">Audio-first VR</div>
  <div id="subtitle"> Imagining the Future of Virtual Environments for Musical Expression</div>
  <div id="bio-container">
    <div class="bio">

      <p><span style="color: #00b8c6; font-size: 18px;">NIME 2018 Workshop, June 3rd, 2018 at Virginia Tech, Blacksburg, VA</span></p>

      <p>We are rapidly approaching the mass adoption of virtual reality as a
      consumer-oriented entertainment medium. Recent breakthroughs in
      low-persistence displays combined with modern head-tracking techniques
      have enabled the design of head-mounted systems that offer plausible
      virtual experiences. The role of sound in establishing a convincing sense
      of immersion in such experiences has been acknowledged since the early
      days of VR. Morton Heilig’s 1960 patent for a stereoscopic television
      apparatus, which is one of the earliest concept designs for a VR system,
      describes the use of ear phones and binaural sound. Ivan Sutherland, who
      designed what is considered to be the first head-mounted display, wrote in
      1965 that despite the availability of excellent audio output devices, the
      computers had not yet been capable of producing meaningful sounds that can
      be integrated into “the ultimate display.” Since then, significant
      advances in digital computing has facilitated the development of real-time
      audio synthesis and spatialization techniques. Modern consumer-grade
      computers are capable of executing complex sound-field synthesis
      algorithms that are suitable for VR applications.</p>

      <p>Leading technology companies today are growingly becoming invested in
      VR audio with new products, such as Facebook’s Spatial Workstation and
      Google’s Resonance Audio, that facilitate the use of Ambisonics and
      binaural audio with popular VR platforms. However, most modern VR systems
      impart an ancillary role to audio, where sounds serve to amplify visual
      immersion. It is up to sound and music researchers to elaborate ways in
      which we can think natively about VR audio, and define the role of sound
      in the ultimate displays of the future. This workshop aims to investigate
      the concept of an “audio-first VR” as a medium for musical expression, and
      identify multimodal experiences that focus on the auditory aspects of
      immersion rather than those that are ocular. Through a day-long workshop
      that involves presentations and demo sessions followed by round table
      discussions, the participants will collectively address such questions as:
      What is a VR experience that originates from sonic phenomena? How do we
      act and interact within such experiences? What are the tools that we have
      or need to create these experiences? What are the implications of an
      audio-first VR for the future of musical expression? As a result of the
      workshop, the participants will collaboratively outline a position on what
      constitutes an audio-first VR from a NIME perspective.</p>

      <p>We seek abstract submissions that focus on the following topics among others:
        <li>Sonic virtual realities
        <li>Immersive sonification
        <li>Virtual interfaces for musical expression (VIMEs)
        <li>Creativity support tools for VR audio
        <li>Visualizing an audio-first VR
        <li>Spatial audio techniques for VR
        <li>Embodied interactions within virtual audio systems
        <li>Applications of computer music theory to VR
        <li>Composing music for VR games and films
        <li>Sonic VR as assistive technology
        <li>Histories of sound in virtual environments

      </p><b>Abstracts of approximately 350 words should be sent as a PDF file
      by May 6th to acamci@umich.edu with "NIME Workshop on Audio-first VR"
      included in the subject line</b>. The accepted abstracts and the
      proceedings of the workshop will be published on the workshop website at
      audio1stVR.github.io. Number of presenting participants will be limited to
      10. However, attendance will be open to non-presenting participants within
      space limits. If you are already presenting a relevant paper or artwork at
      NIME, you are welcome to submit an abstract in relation to that work to
      participate in the workshop.

      <div class="title">Workshop Schedule</div>
      <table class="tg" style="undefined;table-layout: fixed; width: 756px">
<colgroup>
<col style="width: 106px">
<col style="width: 650px">
</colgroup>
  <tr>
    <td class="tg-yw4l">09:00 - 09:20</td>
    <td class="tg-yw4l">Welcome and Introductions</td>
  </tr>
  <tr>
    <td class="tg-b7b8"></td>
    <td class="tg-b7b8">Anıl Çamcı and Rob Hamilton</td>
  </tr>
  <tr>
    <td class="tg-yw4l">09:20 - 09:40</td>
    <td class="tg-yw4l"><a href="abstracts/Mondragon_Blanco_Rivas.pdf" target="new">ECOSONICO: Augmenting Sound and Defining Soundscapes in a Local Interactive Space</a></td>
  </tr>
  <tr>
    <td class="tg-b7b8"></td>
    <td class="tg-b7b8">José M. Mondragón, Adalberto Blanco, and Francisco Rivas</td>
  </tr>
  <tr>
    <td class="tg-yw4l">09:40 - 10:00</td>
    <td class="tg-yw4l"><a href="abstracts/Clarke_Schedel.pdf" target="new">Sonic Thinking in VR: Incorporating Sound into S.T.E.A.M Curriculum and Data-Driven Installations</a></td>
  </tr>
  <tr>
    <td class="tg-b7b8"></td>
    <td class="tg-b7b8">Melissa F. Clarke and Margaret Schedel</td>
  </tr>
  <tr>
    <td class="tg-yw4l">10:00 - 10:20</td>
    <td class="tg-yw4l"><a href="abstracts/Mauro.pdf" target="new">On Standardization, Reproducibility, and Other Demons (of VR)</a></td>
  </tr>
  <tr>
    <td class="tg-b7b8"></td>
    <td class="tg-b7b8">Davide Andrea Mauro</td>
  </tr>
  <tr>
    <td class="tg-yw4l">10:20 - 10:40</td>
    <td class="tg-yw4l"><a href="abstracts/Atherton_Wang.pdf" target="new">Chunity for Audio-first VR</a></td>
  </tr>
  <tr>
    <td class="tg-b7b8"></td>
    <td class="tg-b7b8">Jack Atherton and Ge Wang</td>
  </tr>
  <tr>
    <td class="tg-yw4l">10:40 - 11:00</td>
    <td class="tg-yw4l"><a href="abstracts/Rome.pdf" target="new">Sonic Cyborg Feminist Futures in Extended Realities</a></td>
  </tr>
  <tr>
    <td class="tg-b7b8"></td>
    <td class="tg-b7b8">Rachel Rome</td>
  </tr>
  <tr>
    <td class="tg-yw4l">11:00 - 11:20</td>
    <td class="tg-yw4l"><a href="abstracts/Berthaut.pdf" target="new">Adapting 3D Selection and Manipulation Techniques for Immersive Musical Interaction</a></td>
  </tr>
  <tr>
    <td class="tg-b7b8"></td>
    <td class="tg-b7b8">Florent Berthaut</td>
  </tr>
  <tr>
    <td class="tg-yw4l">11:20 - 11:40</td>
    <td class="tg-yw4l"><a href="abstracts/Simmons.pdf" target="new">What Postmodal Processes Can Teach Us about Existing Mediums</a></td>
  </tr>
  <tr>
    <td class="tg-b7b8"></td>
    <td class="tg-b7b8">Josh Simmons</td>
  </tr>
  <tr>
    <td class="tg-yw4l">11:40 - 12:00</td>
    <td class="tg-yw4l"><a href="abstracts/Mitchusson_Berdahl.pdf" target="new">Enhanced Virtual Reality (EVR) for Live Concert Experiences</a></td>
  </tr>
  <tr>
    <td class="tg-b7b8"></td>
    <td class="tg-b7b8">Chase Mitchusson and Edgar Berdahl</td>
  </tr>
  <tr>
    <td class="tg-yw4l">12:00 - 13:00</td>
    <td class="tg-yw4l">Lunch Break</td>
  </tr>
  <tr>
    <td class="tg-yw4l">13:00 - 14:00</td>
    <td class="tg-yw4l">Demo Sessions</td>
  </tr>
  <tr>
    <td class="tg-yw4l">14:00 - 16:00</td>
    <td class="tg-yw4l">Group Discussion; Outlining a Position on Audio-first VR</td>
  </tr>
</table>

      <div class="title">Workshop Leaders</div>
      <b>Anıl Çamcı</b>  is an Assistant Professor of Performing Arts Technology
      at the University of Michigan. His work investigates new tools and
      theories for multimodal worldmaking using a variety of media ranging from
      electronic music to virtual reality. Previously, he worked at the
      University of Illinois at Chicago, where he led research projects on
      human-computer interaction and immersive audio in virtual reality
      contexts, and Istanbul Technical University, where he founded the Sonic
      Arts Program. He completed his PhD at Leiden University in affiliation
      with the Institute of Sonology in The Hague, and the Industrial Design
      Department at Delft University of Technology. Çamcı’s research has been
      featured in leading journals and conferences throughout the world. He has
      been granted several awards, including the Audio Engineering Society
      Fellowship, and the ACM CHI Artist Grant. [http://anilcamci.com]

      <p><b>Rob Hamilton</b> is a composer and researcher, who explores the converging spaces
      between sound, music, and interaction. His creative practice includes
      mixed-reality performance works built within fully rendered, networked
      game environments, procedural music engines and mobile musical ecosystems.
      His research focuses on the cognitive implications of sonified musical
      gesture and motion and the role of perceived space in the creation and
      enjoyment of sound and music. Dr. Hamilton received his PhD from Stanford
      University’s Center for Computer Research in Music and Acoustics (CCRMA)
      and currently serves as an Assistant Professor of Music and Media at
      Rensselaer Polytechnic Institute. [http://robhamilton.io]</p>

    </div>
  </div>
</body>

</html>
